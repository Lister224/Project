import json
from datetime import datetime
from LLM4sql.mainsql_gemini2 import llm_generate_sql
from search_utils import search_terms_with_like_and_match
from analysis.control_chart import create_control_charts
from analysis.forecast import predict_all_indicators
from LLM4report.Genmini_Analysis_new import generate_report
from pymongo import MongoClient
from dotenv import load_dotenv
from concurrent.futures import ProcessPoolExecutor
import os
import pytz


# åˆå§‹åŒ– MongoDB é€£æ¥
load_dotenv()
MONGO_URI = os.environ.get('mongo_uri')
client = MongoClient(MONGO_URI)
db = client["Finvision"]

# æ¨¡ç³Šæœå°‹
def handle_fuzzy_search(input_data):
    keyword = input_data['rows'][0]['keyWord']
    results = search_terms_with_like_and_match(keyword)
    if results:
        suggestions = [result[1] for result in results]
    else:
        suggestions = []
    return suggestions

# å–å¾—question_id
def get_next_question_id():
    counter = db.counters.find_one_and_update(
        {"_id": "question_id"},
        {"$inc": {"seq": 1}},
        upsert=True,
        return_document=True
    )
    return counter["seq"]

# è§£æ & åˆ†é¡æŸ¥è©¢çµæœ
def parse_and_classify_report(query_report_txt):
    try:
        if query_report_txt.startswith('finvision:'):
            query_report_txt = query_report_txt[len('finvision:'):]
            query_report_type = 'finvision'
        else:
            query_report_type = None

        query_report = json.loads(query_report_txt)
    except json.JSONDecodeError as e:
        return None, str(e), None

    return query_report, None, query_report_type

# å„²å­˜ SQL æŸ¥è©¢çµæœ
def store_sql_data(query_report, query_report_type, question_id, finvision_data, indicator_data):
    query_report['type'] = query_report_type
    if query_report_type == 'finvision':
        data_list = query_report['data']
        indicator_data[question_id] = data_list[0]  # æŒ‡æ¨™æŸ¥è©¢çµæœ
        finvision_data[question_id] = data_list[1:] if len(data_list) > 1 else []
    else:
        return False
    return True

# è§£æ & å„²å­˜ä½¿ç”¨è€…æŸ¥è©¢
def handle_full_query(input_data, user_inputs, finvision_data, indicator_data, llm_reports):
    rows = input_data['rows']
    question_id = get_next_question_id()
    question_date = rows[0]['questionDate']
    question_time = rows[0]['questionTime']
    user_input = rows[0]['questionContent']

    # èª¿ç”¨ LLM ç”Ÿæˆ SQL
    query_report_txt = llm_generate_sql(user_input)
    print(f"Generated Query Report: {query_report_txt}")

    if query_report_txt in ['no', 'LLM_error', 'error_search']:
        return query_report_txt, None

    query_report, json_error, query_report_type = parse_and_classify_report(query_report_txt)
    if json_error:
        print(f'JSON decoding failed: {json_error}')
        return 'system_error', None

    success = store_sql_data(query_report, query_report_type, question_id, finvision_data, indicator_data)
    if not success:
        print('æŸ¥è©¢å ±å‘Šæš«å­˜éŒ¯èª¤')
        return 'system_error', None

    # **æå‰ç”Ÿæˆ LLM åˆ†æå ±å‘Š**
    data = query_report.get('data')
    llm_report = generate_report(data)
    if 'ç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤' in llm_report:
        print("Error generating report.")
        llm_report = 'error'

    # å„²å­˜ LLM åˆ†æå ±å‘Š
    llm_reports[question_id] = llm_report

    # å„²å­˜ç”¨æˆ¶è¼¸å…¥
    tz = pytz.timezone('Asia/Taipei')
    current_time = datetime.now(tz)
    answer_date = current_time.strftime("%Y-%m-%d")
    answer_time = current_time.strftime("%H:%M:%S")

    user_inputs[question_id] = {
        "question_date": question_date,
        "question_time": question_time,
        "user_input": user_input,
        "answer_date": answer_date,
        "answer_time": answer_time,
        "answer_id": question_id
    }

    # âœ… **å›å‚³ LLM å ±å‘Šï¼Œè®“å‰ç«¯ç«‹å³èƒ½æ‹¿åˆ°**
    return question_id, query_report

# é€²è¡Œæ•¸æ“šåˆ†æ
def perform_data_analysis(question_id, query_report):
    print(f"ğŸ” [Process {os.getpid()}] å•Ÿå‹•æ•¸æ“šåˆ†æ - question_id: {question_id}")
    data = query_report.get('data')
    indicator = data[0] if data else None

    if not indicator or indicator == {} or indicator == [{}]:  
        print("Indicator is empty, setting control_chart and forecast to 'error'.")
        control_chart = 'error'
        forecast = 'error'
    else:
        control_chart = create_control_charts(indicator) or 'error'
        forecast = predict_all_indicators(indicator) or 'error'

    return {"control_chart": control_chart, "forecast": forecast}

# èƒŒæ™¯è™•ç†
def background_processing(question_id, query_report, analysis_results, llm_reports, 
                          finvision_data, indicator_data, user_inputs):
    print(f"âš¡ [Background] å•Ÿå‹•æ•¸æ“šåˆ†æ - question_id: {question_id} (Process: {os.getpid()})")

    try:
        with ProcessPoolExecutor(max_workers=2) as executor:
            future = executor.submit(perform_data_analysis, question_id, query_report) #æ•¸æ“šåˆ†æ
            new_analysis_data = future.result()

        # **å–å‡º `llm_report`**
        llm_report = llm_reports.pop(question_id, "error")  # å¦‚æœ `llm_report` ä¸å­˜åœ¨ï¼Œé è¨­ç‚º "error"

        # **åˆä½µ `llm_report`ã€`control_chart` å’Œ `forecast`**
        analysis_results[question_id] = {
            "question_id": question_id,
            "data": {
                "llm_report": llm_report,
                **new_analysis_data
            }
        }

        # **å„²å­˜åˆ° MongoDB**
        db.user_inputs.replace_one(
            {"question_id": question_id},
            {"question_id": question_id, **user_inputs[question_id]},
            upsert=True
        )

        if question_id in finvision_data:
            db.finvision_data.replace_one(
                {"question_id": question_id},
                {"question_id": question_id, "data": finvision_data[question_id]},
                upsert=True
            )

        if question_id in indicator_data:
            db.indicator_data.replace_one(
                {"question_id": question_id},
                {"question_id": question_id, "data": indicator_data[question_id]},
                upsert=True
            )

        db.analysis_results.update_one(
            {"question_id": question_id},
            {"$set": {
                "data": analysis_results[question_id]["data"]
            }},
            upsert=True
        )

        print(f"âœ… [Background] æ•¸æ“šåˆ†æå®Œæˆ - question_id: {question_id} - æ•¸æ“šå·²å­˜å…¥ MongoDB")

    except Exception as e:
        print(f"âŒ [Background] éŒ¯èª¤ - question_id: {question_id} - {str(e)}")



if __name__ == '__main__':
    input_data = {
        "status": "ok",
        "rows": [
            {"keyWord": "æˆ‘è¦åˆ†ææ¯›åˆ©ç‡"}
        ],
        "msg": ""
    }
    print(handle_fuzzy_search(input_data))